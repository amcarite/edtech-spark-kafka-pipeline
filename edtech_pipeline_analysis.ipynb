{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and SQL Pipeline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run some import statements to work with the data\n",
    "from pyspark.sql.types import MapType,StringType\n",
    "from pyspark.sql.functions import json_tuple,from_json,explode,col\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have the json file published into kafka via the run_kafka.sh script, we need to read in that data into pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subscribe to kafka topic - assessments\n",
    "raw_assessments = spark .read .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"kafka:29092\") .option(\"subscribe\", \"assessments\") .option(\"startingOffsets\", \"earliest\") .option(\"endingOffsets\", \"latest\") .load()\n",
    "\n",
    "#cache this new df to avoid some warnings later on\n",
    "raw_assessments.cache()\n",
    "\n",
    "#convert df binary values to strings for human readability\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "\n",
    "#Extract one layer and print Schema to see what we are looking at\n",
    "first_extract = assessments.rdd.map(lambda x: json.loads(x.value)).toDF()\n",
    "\n",
    "first_extract.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: struct (nullable = true)\n",
      " |    |-- attempt: long (nullable = true)\n",
      " |    |-- counts: struct (nullable = true)\n",
      " |    |    |-- all_correct: boolean (nullable = true)\n",
      " |    |    |-- correct: long (nullable = true)\n",
      " |    |    |-- incomplete: long (nullable = true)\n",
      " |    |    |-- incorrect: long (nullable = true)\n",
      " |    |    |-- submitted: long (nullable = true)\n",
      " |    |    |-- total: long (nullable = true)\n",
      " |    |    |-- unanswered: long (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- questions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- options: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- at: string (nullable = true)\n",
      " |    |    |    |    |    |-- checked: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- correct: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |    |-- submitted: long (nullable = true)\n",
      " |    |    |    |-- user_correct: boolean (nullable = true)\n",
      " |    |    |    |-- user_incomplete: boolean (nullable = true)\n",
      " |    |    |    |-- user_result: string (nullable = true)\n",
      " |    |    |    |-- user_submitted: boolean (nullable = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#nested JSON so we need to unravel more than one layer. Use command below\n",
    "extracted_assessments = spark.read.json(assessments.rdd.map(lambda x: x.value))\n",
    "\n",
    "# The above extracted_assessments command is walking through each row and unraveling it\n",
    "# with the lambda/map function, pulling out the values as it unravels  \n",
    "\n",
    "#Take a look at the schema now\n",
    "extracted_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have unravled our Json file, lets make some queries to create a table with necessary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a temporary table under the name \"exams\"\n",
    "extracted_assessments.registerTempTable('exams')\n",
    "\n",
    "##Lets make a sql query that pulls out data that will be useful to the DS team\n",
    "exams_table = \\\n",
    "    spark.sql(\"SELECT \\\n",
    "                  exam_name, \\\n",
    "                  user_exam_id, \\\n",
    "                  sequences.counts.all_correct, \\\n",
    "                  sequences.counts.correct, \\\n",
    "                  sequences.counts.incomplete, \\\n",
    "                  sequences.counts.incorrect, \\\n",
    "                  sequences.counts.submitted, \\\n",
    "                  sequences.counts.total, \\\n",
    "                  sequences.counts.unanswered, \\\n",
    "                  started_at, \\\n",
    "                  base_exam_id, \\\n",
    "                  certification \\\n",
    "              FROM \\\n",
    "                  exams\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write this table to HDFS - Will be useful to DS team as it is a well structured dataset \n",
    "exams_table.write.parquet(\"/tmp/exams_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make some SQL example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(user_exam_id)|\n",
      "+-------------------+\n",
      "|                 16|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Maybe our data science team wants to find out how many times a user took the 'Learning Apache Hadoop' exam/course\n",
    "#using the following query, they can determine this answer\n",
    "\n",
    "spark.sql(\"SELECT count(user_exam_id) FROM exams WHERE exam_name == 'Learning Apache Hadoop'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|           exam_name|    average_score|\n",
      "+--------------------+-----------------+\n",
      "|Learning to Visua...|            100.0|\n",
      "|The Closed World ...|            100.0|\n",
      "|Nulls, Three-valu...|            100.0|\n",
      "|Learning SQL for ...|97.72727272727273|\n",
      "|Introduction to J...|87.59493670886076|\n",
      "|Introduction to A...|83.33333333333333|\n",
      "|Introduction to A...|83.33333333333333|\n",
      "|Getting Ready for...|             80.0|\n",
      "|Cloud Native Arch...|             80.0|\n",
      "|Understanding the...|78.57142857142857|\n",
      "|Introduction to A...| 76.9230769230769|\n",
      "|Beginning Program...|76.58227848101266|\n",
      "|Learning Apache H...|          76.5625|\n",
      "|Refactor a Monoli...|76.47058823529412|\n",
      "|Introduction to H...|             75.0|\n",
      "|Using Storytellin...|             75.0|\n",
      "|Starting a Grails...|             75.0|\n",
      "|Git Fundamentals ...|             75.0|\n",
      "|   Python Epiphanies|74.18300653594771|\n",
      "|Mastering Python ...|             74.0|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query to find out the highest average exam score for each course. \n",
    "scores = spark.sql(\"SELECT  exam_name, user_exam_id, ((sequences.counts.correct / sequences.counts.total)*100) as score FROM exams\")\n",
    "scores.registerTempTable('avg_score')\n",
    "\n",
    "score_table = \\\n",
    "    spark.sql(\"SELECT exam_name, AVG(score) AS average_score FROM avg_score GROUP BY exam_name ORDER BY average_score DESC\")\n",
    "\n",
    "score_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write this to HDFS as well so that the DS can easily access it later\n",
    "score_table.write.parquet(\"/tmp/score_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|           exam_name|students|\n",
      "+--------------------+--------+\n",
      "|        Learning Git|     394|\n",
      "|Introduction to P...|     162|\n",
      "|Intermediate Pyth...|     158|\n",
      "|Introduction to J...|     158|\n",
      "|Learning to Progr...|     128|\n",
      "|Introduction to M...|     119|\n",
      "|Software Architec...|     109|\n",
      "|Beginning C# Prog...|      95|\n",
      "|    Learning Eclipse|      85|\n",
      "|Learning Apache M...|      80|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What are the top 10 most taken courses?\n",
    "stud_count = spark.sql(\"SELECT exam_name, COUNT(*) as students FROM exams GROUP BY exam_name ORDER BY students DESC\")\n",
    "\n",
    "#take a look\n",
    "stud_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add this data to HDFS as well\n",
    "stud_count.write.parquet(\"/tmp/stud_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Assumptions and other notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Assumptions\n",
    "* When counting the number of students per exam, the assumption is that each exam was taken by one student\n",
    "* Assume that each exam attempt also indicates one student taking the class (and vice versa). (i.e. No student took the course \\ without taking the exam, and no student took the exam without also enrolling in the course\n",
    "* Data Science team knows how to work with Apache Hadoop and can pull the dataset from HDFS\n",
    "* Assume the JSON file into our pipeline is at rest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline Flow \n",
    "* docker-compose houses all needed software services\n",
    "* a Kafka topic is created\n",
    "    * kafka topic is named assessments as this dataset is made up of assessments in online courses\n",
    "    * note Zookeeper is included to manage the kafka brokers\n",
    "* We then use Kafkacat to publish the json file in our working directory to the Assessments topic. \n",
    "    * jq is also used in this command to clean up the json structure just a little bit \n",
    "* We then use pySpark to subscribe to the assessments topic and consume the data from the Kafka pipeline\n",
    "* We then use pyspark to unravel the nested json and view the schema \n",
    "* A \"view\" or temp table is created for us to make queries off of\n",
    "* we use sql through pyspark to make queries to that temp table\n",
    "* After we have what we need in a nice structure, we can write these tables to HDFS for longer term storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### After completion\n",
    "* save and close jupyter instance \n",
    "* ^c twice to end instance from terminal window. \n",
    "* run docker-compose down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
